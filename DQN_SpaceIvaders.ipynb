{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "DQN_SpaceIvaders.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kulka193/SpaceInvaders/blob/pb2/DQN_SpaceIvaders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-xEm10v0sK0",
        "colab_type": "text"
      },
      "source": [
        "# SpaceInvaders DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoIaqouC0sK4",
        "colab_type": "text"
      },
      "source": [
        "#### Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wFoHfwE0sK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install wandb -qq\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.models import Sequential, load_model, save_model\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Lambda\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "import os\n",
        "import cv2\n",
        "import wandb\n",
        "import math\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.transform import resize\n",
        "from keras import backend as KB\n",
        "from google.colab import files\n",
        "from random import randrange"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8Kr0dBK_PGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# REQUIRES GOOGLE DRIVE SIGN-IN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u1Ztio00sK-",
        "colab_type": "text"
      },
      "source": [
        "#### Set hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl1IXNSg8Akp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#wandb.init(project=\"qualcomm\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORTkwxJR0sK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q3brgyJSh2D",
        "colab_type": "text"
      },
      "source": [
        "###Hyperparameters & Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jryRsKh80sLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "batch_size = 32\n",
        "n_episodes = 10000\n",
        "output_dir = '/content/drive/My Drive/RL_modified_runs/run4'  #change the directory according to your local file system\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "stack_size = 4 # We stack 4 composite frames in total\n",
        "FRAME_SKIP_SIZE = 4\n",
        "min_observations_to_train = 10000\n",
        "np.random.seed(10)\n",
        "input_nn_shape = (84,84)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2msh_6uh0sLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lsa_CirUqRv",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "###Things to implement to improve performance\n",
        "\n",
        "* Try out Dueling (DDQN )and Adavantage actor-critic variants like A2C and A3C (Big ticket item)\n",
        "* Model based learning methods like planning (Muzero)\n",
        "* Using RAM and spaceship lives as a part of the state-space features -> phi(state)\n",
        "\n",
        "* https://colab.research.google.com/drive/1DggF1gE3FjRu4ftYhYoxQCxLIOaxwVyw\n",
        "* https://becominghuman.ai/beat-atari-with-deep-reinforcement-learning-part-2-dqn-improvements-d3563f665a2c\n",
        "* https://nihit.github.io/resources/spaceinvaders.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5jGS8ihq_hH",
        "colab_type": "text"
      },
      "source": [
        "##Things implemented:\n",
        "* Model-free control using Q-learning\n",
        "* Frame stacking to acheive temporal correlation (modify to look like: https://github.com/gsurma/atari/blob/master/gym_wrappers.py)\n",
        "* Image preprocessing (gray scale, normalization, crop & resize)\n",
        "* Use Experiential Replay\n",
        "* Function Approximation of Q-learning agent using a Conv-Net based network on batchwise state-action pairs\n",
        "* Hyperparameter tuning\n",
        "*  Use of Target network for better convergence\n",
        "* Use RMS Prop optimization with very less learning rate (done)\n",
        "* Use same set of hyperparamaters as DeepMind paper (done)\n",
        "* reward clipping -> (reward clipping and huber loss are  aim to achieve similar things)\n",
        "* Huber loss function (done)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzrGBHldwG-m",
        "colab_type": "text"
      },
      "source": [
        "####Image Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC4-RnybSzSO",
        "colab_type": "text"
      },
      "source": [
        "Utlity functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU338tg1Tl3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(observation):\n",
        "    observation = cv2.cvtColor(cv2.resize(observation, (84, 110)), cv2.COLOR_BGR2GRAY)\n",
        "    observation = observation[26:110,:]\n",
        "    ret, observation = cv2.threshold(observation,1,255,cv2.THRESH_BINARY)\n",
        "    return np.reshape(observation,(84,84,1))\n",
        "\n",
        "color = np.array([210, 164, 74]).mean()\n",
        "\n",
        "def preprocess_frame(obs):\n",
        "    # Crop and resize\n",
        "    img = np.uint8(resize(rgb2gray(obs), input_nn_shape, mode='constant'))\n",
        "    return img\n",
        "\n",
        "def convert_process_buffer(process_buffer):\n",
        "    \"\"\"Converts the list of NUM_FRAMES images in the process buffer\n",
        "    into one training sample\"\"\"\n",
        "    black_buffer = [cv2.resize(cv2.cvtColor(x, cv2.COLOR_RGB2GRAY), (84, 90)) for x in process_buffer]\n",
        "    black_buffer = [x[1:85, :, np.newaxis] for x in black_buffer]\n",
        "    return np.concatenate(black_buffer, axis=2) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgSzaq9_v-_9",
        "colab_type": "text"
      },
      "source": [
        "####Frame Stacking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVTU25n9wDa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# Initialize deque with zero-images one array for each image. Deque is a special kind of queue that deletes last entry when new entry comes in\n",
        "stacked_frames  =  deque([np.zeros(input_nn_shape, dtype=np.int) for i in range(stack_size)], maxlen=stack_size)\n",
        "\n",
        "def stack_frames(stacked_frames, state, is_new_episode):\n",
        "    # Preprocess frame\n",
        "    frame = preprocess_frame(state)\n",
        "    \n",
        "    if is_new_episode:\n",
        "        # Clear our stacked_frames\n",
        "        stacked_frames = deque([np.zeros(input_nn_shape, dtype=np.int) for i in range(stack_size)], maxlen=stack_size)\n",
        "        \n",
        "        # Because we're in a new episode, copy the same frame 4x, apply elementwise maxima\n",
        "        maxframe = np.maximum(frame,frame)\n",
        "        for i in range(stack_size):\n",
        "            stacked_frames.append(frame)\n",
        "        # Stack the frames\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "        \n",
        "    else:\n",
        "        #Since deque append adds t right, we can fetch rightmost element\n",
        "        maxframe=np.maximum(stacked_frames[-1],frame)\n",
        "        # Append frame to deque, automatically removes the oldest frame\n",
        "        stacked_frames.append(frame)\n",
        "\n",
        "        # Build the stacked state (first dimension specifies different frames)\n",
        "        stacked_state = np.stack(stacked_frames, axis=2) \n",
        "    \n",
        "    return stacked_state, stacked_frames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuBPRJwPv7n2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def huber_loss(a, b):\n",
        "    error = a - b\n",
        "    quadratic_term = error*error / 2\n",
        "    linear_term = abs(error) - 1/2\n",
        "    use_linear_term = (abs(error) > 1.0)\n",
        "    use_linear_term = KB.cast(use_linear_term, 'float32')\n",
        "    return use_linear_term * linear_term + (1-use_linear_term) * quadratic_term"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axyEU7Zy0sLT",
        "colab_type": "text"
      },
      "source": [
        "#### Define agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeB54Af70sLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.gamma = 0.99 # decay or discount rate: enables agent to take into account future actions in addition to the immediate ones, but discounted at this rate\n",
        "        self.learning_rate = 0.0005 # rate at which NN adjusts models parameters via SGD to reduce cost \n",
        "        self.model = self._build_model() # private method\n",
        "        self.target_model = self._build_model()\n",
        "        self.num_experiences = 0 \n",
        "    \n",
        "    def _build_model(self):\n",
        "        # neural net to approximate Q-value function:\n",
        "        model = Sequential()\n",
        "        model.add(Lambda(lambda x: x/255.0,input_shape=input_nn_shape + (stack_size,)))\n",
        "        model.add(Conv2D(32, (8,8), strides= (4,4), activation='relu', padding='valid'))\n",
        "        model.add(Conv2D(64, (4,4), strides= (2,2), activation='relu', padding='valid'))\n",
        "        model.add(Conv2D(64, (3,3),  strides= (1,1), activation='relu', padding='valid'))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512, activation='relu')) # 1st hidden layer; states as input\n",
        "        model.add(Dense(64,activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss=huber_loss, optimizer=RMSprop(learning_rate=self.learning_rate, rho=0.95), metrics=['accuracy'])\n",
        "        print(model.summary())\n",
        "        return model\n",
        "\n",
        "    '''\n",
        "    #DQA\n",
        "    def train(self, batch_size): # method that trains NN with experiences sampled from memory\n",
        "        states, actions, rewards, next_states, dones = self.sample_memories(batch_size) # sample a minibatch from memory\n",
        "        targets = np.zeros((batch_size, action_size))\n",
        "        for i in range(batch_size):\n",
        "            targets[i] = self.predict_returns(states[i], target=False)[0] #self.model.predict(states[i], batch_size = 1)\n",
        "            Q_sa = self.predict_returns(next_states[i], target=True)[0]  #self.model.predict(next_states[i], batch_size = 1)\n",
        "            targets[i, actions[i]] = rewards[i]\n",
        "            if not dones[i]:\n",
        "                targets[i, actions[i]] += self.gamma * np.max(Q_sa)\n",
        "        loss = self.model.train_on_batch(states, targets)\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon -((1-self.epsilon_min) * self.epsilon_decay))\n",
        "    '''\n",
        "\n",
        "\n",
        "    def train_network(self, data):\n",
        "        x_train = []\n",
        "        t_train = []\n",
        "        batch_size = 0\n",
        "        for batchpoint in data:\n",
        "            this_s = batchpoint[0]\n",
        "            action = batchpoint[1]\n",
        "            reward = batchpoint[2]\n",
        "            next_s = batchpoint[3]\n",
        "            done = batchpoint[4]\n",
        "            x_train.append(this_s.astype(np.float64))\n",
        "            Q_sa = self.predict_returns(next_s, target=True)[0]\n",
        "            t = self.predict_returns(this_s, target=False)[0]\n",
        "            t[action] = reward\n",
        "            if not done:\n",
        "                t[action] += self.gamma * np.max(Q_sa)\n",
        "            t_train.append(t)\n",
        "            batch_size += 1\n",
        "        x_train.asarray(np.float64).squeeze()\n",
        "        t_train.asarray(np.float64).squeeze()\n",
        "        print('X train shape', x_train.shape)\n",
        "        print('Y train shape', t_train.shape)\n",
        "        self.model.fit(x_train, t_train, batch_size, nb_epoch=1)\n",
        "\n",
        "    def predict_returns(self, s_t, target=False):\n",
        "        state = np.reshape(s_t, (1,) + s_t.shape)\n",
        "        if target:\n",
        "            return self.target_model.predict(state, batch_size = 1)\n",
        "        return self.model.predict(state, batch_size = 1)\n",
        "\n",
        "    '''\n",
        "    #DQA\n",
        "    def sample_memories(self, batch_size):\n",
        "        if self.num_experiences < batch_size:\n",
        "            batch = random.sample(self.memory, self.num_experiences)\n",
        "        else:\n",
        "            batch = random.sample(self.memory, batch_size)\n",
        "        # Maps each experience in batch in batches of states, actions, rewards and new states\n",
        "        s_batch, a_batch, r_batch, d_batch, s2_batch = list(map(np.array, list(zip(*batch))))\n",
        "        return s_batch, a_batch, r_batch, d_batch, s2_batch\n",
        "    '''\n",
        "\n",
        "\n",
        "    '''\n",
        "    #DQA\n",
        "    def act(self, state):\n",
        "        if np.random.random() <= self.epsilon:\n",
        "            a = random.randrange(self.action_size)\n",
        "        else:\n",
        "            a = np.argmax(predict_returns(state)[0])#np.argmax(self.model.predict(state.reshape(shape=(1,)+state.shape))[0])\n",
        "        return a\n",
        "    '''\n",
        "        \n",
        "    def save(self, name):\n",
        "        save_model(self.model, name)\n",
        "\n",
        "    def target_train(self):\n",
        "        self.target_model.model.set_weights(self.model.model.get_weights())\n",
        "\n",
        "    def load(self, name, custom_objects):\n",
        "        return load_model(name, custom_objects)\n",
        "        #self.model.load_weights(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzAMN12YUvJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, max_memory_size, action_size, state_size, epsilon_min=0.1, epsilon_decay=1e-04):\n",
        "        self.max_memory_size = max_memory_size\n",
        "        self.memory = deque(maxlen=max_memory_size)\n",
        "        self.num_experiences = 0\n",
        "        self.action_size = action_size\n",
        "        self.state_size = state_size\n",
        "        self.nn = DQN(state_size, action_size)\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon=1.0\n",
        "        self.epsilon_min=epsilon_min\n",
        "\n",
        "\n",
        "    def get_size(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        if len(self.memory) < self.memory.maxlen: # list of previous experiences, enabling re-training later\n",
        "            self.num_experiences += 1\n",
        "        else:\n",
        "            self.memory.popleft()\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def act(self, state):\n",
        "        if np.random.random() <= self.epsilon:\n",
        "            a = random.randrange(self.action_size)\n",
        "        else:\n",
        "            a = np.argmax(predict_returns(state)[0])#np.argmax(self.model.predict(state.reshape(shape=(1,)+state.shape))[0])\n",
        "        return a\n",
        "\n",
        "    def sample_memories(self, batch_size):\n",
        "        batch = []\n",
        "        for i in range(batch_size):\n",
        "            batch.append(self.memory[randrange(0, len(self.memory))])\n",
        "        return np.asarray(batch)\n",
        "    \n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon -((1-self.epsilon_min) * self.epsilon_decay))\n",
        "\n",
        "    def set_target(self):\n",
        "        self.nn.target_train()\n",
        "\n",
        "    def train_agent(self, batch_size):\n",
        "        batch_data = self.sample_memories(batch_size=batch_size)\n",
        "        self.nn.train_network(batch_data)\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon -((1-self.epsilon_min) * self.epsilon_decay))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mVW8uvz0sLX",
        "colab_type": "text"
      },
      "source": [
        "#### Interact with environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8zfx0_B0sLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJBaWgKSv4vt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7UpMja54v9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpmZXxJF43AR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxXdXUFT7TTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rGviKjg7_Or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display = Display(visible=0, size=(1400,900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKb7EU9R5WP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgV1rWhv6Fnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybkzu-z9cfqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cumulative_reward = 0\n",
        "\n",
        "\n",
        "def evaluate(episodic_reward, ep):\n",
        "  '''\n",
        "  Takes in the reward for an episode, calculates the cumulative_avg_reward\n",
        "    and logs it in wandb. If episode > 100, stops logging scores to wandb.\n",
        "    Called after playing each episode. See example below.\n",
        "\n",
        "  Arguments:\n",
        "    episodic_reward - reward received after playing current episode\n",
        "  '''\n",
        "  global cumulative_reward\n",
        "\n",
        "  # your models will be evaluated on 100-episode average reward\n",
        "  # therefore, we stop logging after 100 episodes\n",
        "  # log total reward received in this episode to wandb\n",
        "  #wandb.log({'episodic_reward': episodic_reward})\n",
        "\n",
        "  # add reward from this episode to cumulative_reward\n",
        "  cumulative_reward += episodic_reward\n",
        "\n",
        "  # calculate the cumulative_avg_reward\n",
        "  # this is the metric your models will be evaluated on\n",
        "  if (ep + 1) % 100 == 0:\n",
        "      cumulative_avg_reward = cumulative_reward/100\n",
        "      print('cumulative_avg_reward: ',cumulative_avg_reward)\n",
        "      # log cumulative_avg_reward over all episodes played so far\n",
        "      #wandb.log({'cumulative_avg_reward': cumulative_avg_reward})\n",
        "      cumulative_reward = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBITewME0sLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "batch_size = 32\n",
        "no_op_steps =10  #number of steps at the start of each episode for which NoOP  is performed\n",
        "C=10   # target network update frequency\n",
        "start_ep=0  # keep this 0 if you're training a fresh model\n",
        "load = True  # keep this False if you havent saved your graph\n",
        "if load:\n",
        "    load_dir = '/content/drive/My Drive/RL_modified_runs/run3'\n",
        "    agent.model = agent.load(os.path.join(load_dir,'model_10000.h5'), custom_objects={'huber_loss': huber_loss})  #model you want to load #override\n",
        "    start_ep = 0  #traiing to resume from which episode? #override\n",
        "    agent.epsilon = 1.0  # value of epsilon when the training was left off previously #override\n",
        "    print('resuming training... \\n')\n",
        "for e in range(start_ep,n_episodes + 1): # iterate over episodes of gameplay\n",
        "    state = env.reset() # reset state at start of each new episode of the game\n",
        "    #state = preprocess_frame(state)\n",
        "    done = False\n",
        "    agent.state_size = np.reshape(state,[-1,]).shape[0]\n",
        "    #print(agent.state_size)\n",
        "    for _ in range(no_op_steps):\n",
        "        state, _, _, _ = env.step(0)\n",
        "    state,stacked_frames= stack_frames(stacked_frames,state,True)\n",
        "    time = 0 # time represents a frame of the episode\n",
        "    total_rewards = 0\n",
        "    while not done:\n",
        "        action = agent.act(state) # action is either 0 or 1 (move cart left or right); decide on one or other here\n",
        "        reward = 0\n",
        "        for _ in range(0,FRAME_SKIP_SIZE-1):  # perform frame skipping\n",
        "            next_state, r_frame, done, _ = env.step(action) # agent interacts with env, gets feedback;\n",
        "            reward += r_frame\n",
        "            if done:  # if episode ends: \n",
        "                print(\"episode: {}/{}, frames observed: {}, e: {:.2}\" # print the episode's score and agent's epsilon\n",
        "                  .format(e, n_episodes-1, total_frames_seen, agent.epsilon))\n",
        "                break\n",
        "        # reward = np.sign(reward)       # reward clipping\n",
        "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)  #stack last 4 frames seen\n",
        "        agent.remember(state, action, reward, next_state, done) # store experience \n",
        "        state = next_state  # set \"current state\" for upcoming iteration to the current next state        \n",
        "        time += 1\n",
        "        total_rewards += reward\n",
        "        total_frames_seen += 4\n",
        "    print('total rewards this episode: ', total_rewards)\n",
        "    evaluate(total_rewards, e)\n",
        "    if agent.size() > min_observations_to_train:\n",
        "        agent.train(batch_size) # train the agent by replaying the experiences of the episode\n",
        "    if e % C == 0 and e != 0:\n",
        "        agent.target_train()   #target network training every C updates for stability\n",
        "    if e % 500 == 0 and e!=0 :\n",
        "        show_video()\n",
        "        agent.save(os.path.join(output_dir,\"model_\"+str(int(e))+\".h5\"))\n",
        "        #wandb.save(os.path.join(wandb.run.dir, \"model.h5\"))\n",
        "        # agent.save(output_dir + \"weights_\"  + '{:04d}'.format(e) + \".hdf5\")\n",
        "env.close()\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2RagqWZkgXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_frames(current, obs):\n",
        "    return np.append(current[1:], [obs], axis=0)\n",
        "\n",
        "def preprocess_frame(obs):\n",
        "    # Crop and resize\n",
        "    img = np.uint8(resize(rgb2gray(obs), input_nn_shape, mode='constant'))\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS1byo6Q-exY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PlayGame(object):\n",
        "    def __init__(self, batch_size, n_epsiodes, update_freq, target_update_freq, load=False, noOp=30):\n",
        "        self.batch_size = batch_size\n",
        "        self.n_episodes = n_episodes\n",
        "        self.load = load\n",
        "        self.target_update_freq = target_update_freq\n",
        "        self.update_freq = update_freq\n",
        "        self.noOp = noOp\n",
        "        if not load:\n",
        "            self.start_ep = 0\n",
        "        self._set_env()\n",
        "        self.agent = Agent(max_memory_size=500000, state_size=self.env.observation_space.shape[0], action_size=self.env.action_space.n)\n",
        "    \n",
        "    def _set_env(self):\n",
        "        self.env = wrap_env(gym.make(\"SpaceInvaders-v0\"))\n",
        "\n",
        "\n",
        "    def game(self, show_every):\n",
        "        total_frames_seen = 0\n",
        "        for e in range(self.start_ep, self.n_episodes + 1): # iterate over episodes of gameplay\n",
        "            state = self.env.reset() # reset state at start of each new episode of the game\n",
        "            #state = preprocess_frame(state)\n",
        "            done = False\n",
        "            self.agent.state_size = np.reshape(state,[-1,]).shape[0]\n",
        "            #print(agent.state_size)\n",
        "            for _ in range(self.noOp):\n",
        "                state, _, _, _ = self.env.step(0)\n",
        "            state = preprocess_frame(state)\n",
        "            current_framed_state = np.array([state] *stack_size)\n",
        "            #current_framed_state = current_framed_state.reshape([input_nn_shape[0], input_nn_shape[1]] + [stack_size])\n",
        "            total_frames_seen += 1\n",
        "            time_steps = 0\n",
        "            total_rewards = 0\n",
        "            frame_counter = 0\n",
        "            while not done:\n",
        "                action = self.agent.act(current_framed_state)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                total_frames_seen += 1\n",
        "                next_state = preprocess_frame(next_state)\n",
        "                if done:  # if episode ends: \n",
        "                    print(\"episode: {}/{}, frames observed: {}, frames till now: {}, e: {:.2} , total_rewards\" # print the episode's score and agent's epsilon\n",
        "                      .format(e, n_episodes-1, frame_counter, total_frames_seen, self.agent.epsilon, total_rewards))\n",
        "                    break\n",
        "                print(current_framed_state[1:].shape)\n",
        "                next_framed_state = get_frames(current_framed_state, next_state)\n",
        "                frame_counter += 1\n",
        "                self.agent.remember(current_framed_state, action, reward, next_framed_state, done)\n",
        "                if time_steps % self.update_freq == 0 and self.agent.get_size() >= min_observations_to_train:\n",
        "                    self.agent.train_agent(self.batch_size)\n",
        "                    if total_frames_seen % self.target_update_freq == 0 and total_frames_seen >= min_observations_to_train:\n",
        "                        self.agent.set_target()\n",
        "                current_framed_state = next_framed_state\n",
        "                total_rewards += reward\n",
        "                time_steps += 1\n",
        "                if e % show_every == 0 and e!=0 :\n",
        "                    show_video()\n",
        "                    #agent.save(os.path.join(output_dir,\"model_\"+str(int(e))+\".h5\"))\n",
        "                    self.agent.nn.save(os.path.join(output_dir,\"model_\"+str(int(e))+\".h5\"))\n",
        "        self.env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kzjm4KAZDyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f1LdCMH-n6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "playgame = PlayGame(batch_size=32, n_epsiodes=10000, update_freq=4, target_update_freq=10000)\n",
        "playgame.game(show_every=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBX6_mfEYDcl",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEneVl5SWMJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(os.path.join(output_dir,\"model_\"+str(int(e))+\".h5\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RtFVsSaXXgF",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9rTPo1X1F4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent.save('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMD0ADsqbkjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent.load('model.h5')\n",
        "agent.model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45S-IKR_pFmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent.save(os.path.join(wandb.run.dir, \"model.h5\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRTc7fXuqUs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfUOqT-nqapv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wandb.save(os.path.join(wandb.run.dir, \"model.h5\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skNf0in5qnbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = [(0,1),(1,2)]\n",
        "s.remove((0,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orluWV1PqvPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eoQs8ddhhjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.reset().shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVc9WT2krZs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nkdMlfJIJ4Y",
        "colab_type": "text"
      },
      "source": [
        "Evaluate and play"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOuhtZn7IPbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "api = wandb.Api()\n",
        "agent.load('/content/wandb/run-20200422_031633-24ees0sk/model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}