{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "DQN_SpaceIvaders.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-xEm10v0sK0",
        "colab_type": "text"
      },
      "source": [
        "# SpaceInvaders DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoIaqouC0sK4",
        "colab_type": "text"
      },
      "source": [
        "#### Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wFoHfwE0sK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install wandb -qq\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.models import Sequential, load_model, save_model\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Lambda\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "import os\n",
        "import cv2\n",
        "import wandb\n",
        "import math\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.transform import resize\n",
        "from keras import backend as KB\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u1Ztio00sK-",
        "colab_type": "text"
      },
      "source": [
        "#### Set hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl1IXNSg8Akp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#wandb.init(project=\"qualcomm\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORTkwxJR0sK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('SpaceInvaders-v0') # initialize environment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jryRsKh80sLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state_size = env.observation_space.shape[0]\n",
        "state_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk0WcIuK0sLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "action_size = env.action_space.n\n",
        "action_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4MIKcKZ0sLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "n_episodes = 10000\n",
        "output_dir = 'model_output/spaceinvaders/'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "stack_size = 3 # We stack 3 composite frames in total\n",
        "total_frames_seen = 0\n",
        "final_epsilon = 0.1\n",
        "\n",
        "min_observations_to_train = 10000\n",
        "np.random.seed(12345)\n",
        "TAU=0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2msh_6uh0sLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.observation_space.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lsa_CirUqRv",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "###Things to implement to improve performance\n",
        "1.  Use Target network\n",
        "2.  Use RMS Prop with very less learning rate (done)\n",
        "3. Use same hyperparamaters as DeepMind paper\n",
        "4. Frame skipping and reward clipping -> (reward clipping and huber loss are mutually exclusive)\n",
        "6. Huber loss optimization (done)\n",
        "7. Try out Dueling (DDQN )and A2C/A3C\n",
        "8. Using RAM and spaceship lives as a part of the state-space features\n",
        "\n",
        "* https://colab.research.google.com/drive/1DggF1gE3FjRu4ftYhYoxQCxLIOaxwVyw\n",
        "* https://becominghuman.ai/beat-atari-with-deep-reinforcement-learning-part-2-dqn-improvements-d3563f665a2c\n",
        "* https://nihit.github.io/resources/spaceinvaders.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5jGS8ihq_hH",
        "colab_type": "text"
      },
      "source": [
        "##Things implemented:\n",
        "* Frame stacking to acheive temporal correlation (modify to look like: https://github.com/gsurma/atari/blob/master/gym_wrappers.py)\n",
        "* Image preprocessing ( gray scale, normalization, crop & resize)\n",
        "* Use Experiential Replay buffers\n",
        "* Function Approximation of Q-learning agent using a Conv-Net based network on batchwise state-action pairs\n",
        "* Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzrGBHldwG-m",
        "colab_type": "text"
      },
      "source": [
        "####Image Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB-vK7dBqomT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_nn_shape = (84,84)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU338tg1Tl3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(observation):\n",
        "    observation = cv2.cvtColor(cv2.resize(observation, (84, 110)), cv2.COLOR_BGR2GRAY)\n",
        "    observation = observation[26:110,:]\n",
        "    ret, observation = cv2.threshold(observation,1,255,cv2.THRESH_BINARY)\n",
        "    return np.reshape(observation,(84,84,1))\n",
        "\n",
        "color = np.array([210, 164, 74]).mean()\n",
        "\n",
        "def preprocess_frame(obs):\n",
        "    # Crop and resize\n",
        "    img = np.uint8(resize(rgb2gray(obs), input_nn_shape, mode='constant'))\n",
        "    return img\n",
        "\n",
        "def convert_process_buffer(process_buffer):\n",
        "    \"\"\"Converts the list of NUM_FRAMES images in the process buffer\n",
        "    into one training sample\"\"\"\n",
        "    black_buffer = [cv2.resize(cv2.cvtColor(x, cv2.COLOR_RGB2GRAY), (84, 90)) for x in process_buffer]\n",
        "    black_buffer = [x[1:85, :, np.newaxis] for x in black_buffer]\n",
        "    return np.concatenate(black_buffer, axis=2) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgSzaq9_v-_9",
        "colab_type": "text"
      },
      "source": [
        "####Frame Stacking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVTU25n9wDa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# Initialize deque with zero-images one array for each image. Deque is a special kind of queue that deletes last entry when new entry comes in\n",
        "stacked_frames  =  deque([np.zeros(input_nn_shape, dtype=np.int) for i in range(stack_size)], maxlen=stack_size)\n",
        "\n",
        "def stack_frames(stacked_frames, state, is_new_episode):\n",
        "    # Preprocess frame\n",
        "    frame = preprocess_frame(state)\n",
        "    \n",
        "    if is_new_episode:\n",
        "        # Clear our stacked_frames\n",
        "        stacked_frames = deque([np.zeros(input_nn_shape, dtype=np.int) for i in range(stack_size)], maxlen=stack_size)\n",
        "        \n",
        "        # Because we're in a new episode, copy the same frame 4x, apply elementwise maxima\n",
        "        maxframe = np.maximum(frame,frame)\n",
        "        for i in range(stack_size):\n",
        "            stacked_frames.append(frame)\n",
        "        # Stack the frames\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "        \n",
        "    else:\n",
        "        #Since deque append adds t right, we can fetch rightmost element\n",
        "        maxframe=np.maximum(stacked_frames[-1],frame)\n",
        "        # Append frame to deque, automatically removes the oldest frame\n",
        "        stacked_frames.append(frame)\n",
        "\n",
        "        # Build the stacked state (first dimension specifies different frames)\n",
        "        stacked_state = np.stack(stacked_frames, axis=2) \n",
        "    \n",
        "    return stacked_state, stacked_frames\n",
        "\n",
        "\n",
        "def temp_bufferize(env):\n",
        "    for i in range(NUM_FRAMES):\n",
        "        temp_observation, temp_reward, temp_done, _ = env.step(predict_movement)\n",
        "        reward += temp_reward\n",
        "        self.process_buffer.append(temp_observation)\n",
        "        done = done | temp_done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuBPRJwPv7n2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def huber_loss(a, b):\n",
        "    error = a - b\n",
        "    quadratic_term = error*error / 2\n",
        "    linear_term = abs(error) - 1/2\n",
        "    use_linear_term = (abs(error) > 1.0)\n",
        "    use_linear_term = KB.cast(use_linear_term, 'float32')\n",
        "    return use_linear_term * linear_term + (1-use_linear_term) * quadratic_term"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axyEU7Zy0sLT",
        "colab_type": "text"
      },
      "source": [
        "#### Define agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeB54Af70sLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, epsilon_norm):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=800000) # double-ended queue; acts like list, but elements can be added/removed from either end\n",
        "        self.gamma = 0.99 # decay or discount rate: enables agent to take into account future actions in addition to the immediate ones, but discounted at this rate\n",
        "        self.epsilon = 1.0 # exploration rate: how much to act randomly; more initially than later due to epsilon decay\n",
        "        self.epsilon_min = 0.1 # minimum amount of random exploration permitted\n",
        "        self.epsilon_decay = 1e-04 #(self.epsilon - self.epsilon_min) / (1 + math.log(epsilon_norm))  with decay=0.9995 # decrease number of random explorations as the agent's performance (hopefully) improves over time\n",
        "        print('Choosing epsilon decay rate=',self.epsilon_decay)\n",
        "        self.learning_rate = 0.00025 # rate at which NN adjusts models parameters via SGD to reduce cost \n",
        "        self.model = self._build_model() # private method\n",
        "        self.target_model = self._build_model()\n",
        "        self.num_experiences = 0 \n",
        "    \n",
        "    def _build_model(self):\n",
        "        # neural net to approximate Q-value function:\n",
        "        model = Sequential()\n",
        "        model.add(Lambda(lambda x: x/255.0,input_shape=input_nn_shape + (stack_size,)))\n",
        "        model.add(Conv2D(32, (8,8), strides= (4,4), activation='relu', padding='same'))\n",
        "        model.add(Conv2D(64, (4,4), strides= (2,2), activation='relu', padding='same'))\n",
        "        model.add(Conv2D(64, (3,3),  strides= (1,1), activation='relu', padding='same'))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dropout(0.1))\n",
        "        model.add(Dense(512, activation='relu')) # 1st hidden layer; states as input\n",
        "        model.add(Dense(64,activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss=huber_loss, optimizer=RMSprop(learning_rate=self.learning_rate, rho=0.95), metrics=['accuracy'])\n",
        "        print(model.summary())\n",
        "        return model\n",
        "    \n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        if self.num_experiences < self.memory.maxlen:\n",
        "            self.memory.append((state, action, reward, next_state, done)) # list of previous experiences, enabling re-training later\n",
        "            self.num_experiences += 1\n",
        "        else:\n",
        "            self.memory.popleft()\n",
        "            self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def size(self):\n",
        "        return self.num_experiences\n",
        "\n",
        "    def train(self, batch_size): # method that trains NN with experiences sampled from memory\n",
        "        states, actions, rewards, next_states, dones = self.sample_memories(batch_size) # sample a minibatch from memory\n",
        "        targets = np.zeros((batch_size, action_size))\n",
        "        for i in range(batch_size):\n",
        "            targets[i] = self.predict_returns(states[i], target=False)[0] #self.model.predict(states[i], batch_size = 1)\n",
        "            Q_sa = self.predict_returns(next_states[i], target=True)[0]  #self.model.predict(next_states[i], batch_size = 1)\n",
        "            targets[i, actions[i]] = rewards[i]\n",
        "            if not dones[i]:\n",
        "                targets[i, actions[i]] += self.gamma * np.max(Q_sa)\n",
        "        loss = self.model.train_on_batch(states, targets)\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon -((1-self.epsilon_min) * self.epsilon_decay))\n",
        "        '''\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            td_target = reward\n",
        "            if not done:\n",
        "                Q_next = self.model.predict(next_state)[0]\n",
        "                td_target = reward + self.gamma * np.max(Q_next)\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0, action] = td_target\n",
        "            self.model.fit(state, [target_f], epochs=1, verbose=0)\n",
        "        #self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay) #max(self.epsilon_min, self.epsilon - ((self.epsilon_decay)**2.5) * self.epsilon)\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon -((1-self.epsilon_min) * self.epsilon_decay))\n",
        "        '''\n",
        "    \n",
        "    def predict_returns(self, s_t, target=False):\n",
        "        state = np.reshape(s_t, (1,) + s_t.shape)\n",
        "        if target:\n",
        "            return self.target_model.predict(state, batch_size = 1)\n",
        "        return self.model.predict(state, batch_size = 1)\n",
        "\n",
        "\n",
        "    def sample_memories(self, batch_size):\n",
        "        if self.num_experiences < batch_size:\n",
        "            batch = random.sample(self.memory, self.num_experiences)\n",
        "        else:\n",
        "            batch = random.sample(self.memory, batch_size)\n",
        "        # Maps each experience in batch in batches of states, actions, rewards and new states\n",
        "        s_batch, a_batch, r_batch, d_batch, s2_batch = list(map(np.array, list(zip(*batch))))\n",
        "        return s_batch, a_batch, r_batch, d_batch, s2_batch\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.random() <= self.epsilon:\n",
        "            a = random.randrange(self.action_size)\n",
        "        else:\n",
        "            a = np.argmax(self.predict_returns(state)[0])#np.argmax(self.model.predict(state.reshape(shape=(1,)+state.shape))[0])\n",
        "        return a\n",
        "    \n",
        "    def save(self, name):\n",
        "        save_model(self.model, name)\n",
        "\n",
        "    def target_train(self):\n",
        "        model_weights = self.model.get_weights()\n",
        "        target_model_weights = self.target_model.get_weights()\n",
        "        for i in range(len(model_weights)):\n",
        "            target_model_weights[i] = TAU * model_weights[i] + (1 - TAU) * target_model_weights[i]\n",
        "        self.target_model.set_weights(target_model_weights)\n",
        "\n",
        "    def load(self, name, custom_objects):\n",
        "        return load_model(name, custom_objects)\n",
        "        #self.model.load_weights(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mVW8uvz0sLX",
        "colab_type": "text"
      },
      "source": [
        "#### Interact with environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8zfx0_B0sLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent = DQNAgent(state_size, action_size, n_episodes) # initialise agent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJBaWgKSv4vt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7UpMja54v9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpmZXxJF43AR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxXdXUFT7TTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rGviKjg7_Or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display = Display(visible=0, size=(1400,900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKb7EU9R5WP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgV1rWhv6Fnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = wrap_env(gym.make(\"SpaceInvaders-v0\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybkzu-z9cfqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cumulative_reward = 0\n",
        "\n",
        "\n",
        "def evaluate(episodic_reward, ep):\n",
        "  '''\n",
        "  Takes in the reward for an episode, calculates the cumulative_avg_reward\n",
        "    and logs it in wandb. If episode > 100, stops logging scores to wandb.\n",
        "    Called after playing each episode. See example below.\n",
        "\n",
        "  Arguments:\n",
        "    episodic_reward - reward received after playing current episode\n",
        "  '''\n",
        "  global cumulative_reward\n",
        "\n",
        "  # your models will be evaluated on 100-episode average reward\n",
        "  # therefore, we stop logging after 100 episodes\n",
        "  # log total reward received in this episode to wandb\n",
        "  #wandb.log({'episodic_reward': episodic_reward})\n",
        "\n",
        "  # add reward from this episode to cumulative_reward\n",
        "  cumulative_reward += episodic_reward\n",
        "\n",
        "  # calculate the cumulative_avg_reward\n",
        "  # this is the metric your models will be evaluated on\n",
        "  if (ep + 1) % 100 == 0:\n",
        "      cumulative_avg_reward = cumulative_reward/100\n",
        "      print('cumulative_avg_reward: ',cumulative_avg_reward)\n",
        "      # log cumulative_avg_reward over all episodes played so far\n",
        "      #wandb.log({'cumulative_avg_reward': cumulative_avg_reward})\n",
        "      cumulative_reward = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBITewME0sLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "no_op_steps = 30\n",
        "C=10000\n",
        "for e in range(n_episodes + 1): # iterate over episodes of gameplay\n",
        "    state = env.reset() # reset state at start of each new episode of the game\n",
        "    #state = preprocess_frame(state)\n",
        "    done = False\n",
        "    #agent.model = agent.load('model_1000.h5',custom_objects={'huber_loss':huber_loss})\n",
        "    agent.state_size = np.reshape(state,[-1,]).shape[0]\n",
        "    #print(agent.state_size)\n",
        "    for _ in range(random.randint(1, no_op_steps)):\n",
        "            state, _, _, _ = env.step(1)\n",
        "    state,stacked_frames= stack_frames(stacked_frames,state,True)\n",
        "    #state = np.reshape(state, (1,)+ state.shape)\n",
        "    time = 0 # time represents a frame of the episode\n",
        "    total_rewards = 0\n",
        "    while not done:\n",
        "        action = agent.act(state) # action is either 0 or 1 (move cart left or right); decide on one or other here\n",
        "        next_state, reward, done, _ = env.step(action) # agent interacts with env, gets feedback; 4 state data points, e.g., pole angle, cart position \n",
        "        # reward = np.sign(reward)       # reward clipping\n",
        "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "        agent.remember(state, action, reward, next_state, done) # store experience \n",
        "        state = next_state  # set \"current state\" for upcoming iteration to the current next state        \n",
        "        if done:  # if episode ends: \n",
        "            print(\"episode: {}/{}, frames observed: {}, e: {:.2}\" # print the episode's score and agent's epsilon\n",
        "                  .format(e, n_episodes-1, total_frames_seen, agent.epsilon))\n",
        "            break\n",
        "        time += 1\n",
        "        total_rewards += reward\n",
        "        total_frames_seen += 1\n",
        "    print('total rewards this episode: ', total_rewards)\n",
        "    evaluate(total_rewards, e)\n",
        "    if agent.size() > min_observations_to_train:\n",
        "        agent.train(batch_size) # train the agent by replaying the experiences of the episode\n",
        "    if total_frames_seen % C == 0:\n",
        "        agent.target_train()   #target network training every C updates for stability\n",
        "    if e % 100 == 0 and e!=0 :\n",
        "        show_video()\n",
        "        agent.save(\"model_\"+str(int(e))+\".h5\")\n",
        "        #wandb.save(os.path.join(wandb.run.dir, \"model.h5\"))\n",
        "        # agent.save(output_dir + \"weights_\"  + '{:04d}'.format(e) + \".hdf5\")\n",
        "    '''\n",
        "    try:\n",
        "        if e % 1000 == 0 and e!=0:\n",
        "            files.download(\"model_\"+str(int(e))+\".h5\")\n",
        "    except:\n",
        "        print('Couldnt download')\n",
        "        pass\n",
        "    '''\n",
        "env.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEneVl5SWMJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9rTPo1X1F4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent.save('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3UI-15PTSuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "84*84"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JijwgP-sqAsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMD0ADsqbkjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent.load('model.h5')\n",
        "agent.model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45S-IKR_pFmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent.save(os.path.join(wandb.run.dir, \"model.h5\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRTc7fXuqUs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfUOqT-nqapv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wandb.save(os.path.join(wandb.run.dir, \"model.h5\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skNf0in5qnbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = [(0,1),(1,2)]\n",
        "s.remove((0,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orluWV1PqvPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eoQs8ddhhjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.reset().shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVc9WT2krZs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nkdMlfJIJ4Y",
        "colab_type": "text"
      },
      "source": [
        "Evaluate and play"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOuhtZn7IPbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "api = wandb.Api()\n",
        "agent.load('/content/wandb/run-20200422_031633-24ees0sk/model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aXbpsVkIPl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cumulative_reward = 0\n",
        "episode = 0\n",
        "\n",
        "def evaluate_play(episodic_reward, reset=False):\n",
        "  '''\n",
        "  Takes in the reward for an episode, calculates the cumulative_avg_reward\n",
        "    and logs it in wandb. If episode > 100, stops logging scores to wandb.\n",
        "    Called after playing each episode. See example below.\n",
        "\n",
        "  Arguments:\n",
        "    episodic_reward - reward received after playing current episode\n",
        "  '''\n",
        "  global episode\n",
        "  global cumulative_reward\n",
        "  if reset:\n",
        "    cumulative_reward = 0\n",
        "    episode = 0\n",
        "    \n",
        "  episode += 1\n",
        "  print(\"Episode: %d\"%(episode))\n",
        "\n",
        "  # your models will be evaluated on 100-episode average reward\n",
        "  # therefore, we stop logging after 100 episodes\n",
        "  if (episode > 100):\n",
        "    print(\"Scores from episodes > 100 won't be logged in wandb.\")\n",
        "    return\n",
        "\n",
        "  # log total reward received in this episode to wandb\n",
        "  wandb.log({'episodic_reward': episodic_reward})\n",
        "\n",
        "  # add reward from this episode to cumulative_reward\n",
        "  cumulative_reward += episodic_reward\n",
        "\n",
        "  # calculate the cumulative_avg_reward\n",
        "  # this is the metric your models will be evaluated on\n",
        "  cumulative_avg_reward = cumulative_reward/episode\n",
        "\n",
        "  # log cumulative_avg_reward over all episodes played so far\n",
        "  wandb.log({'cumulative_avg_reward': cumulative_avg_reward})\n",
        "\n",
        "  return cumulative_avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om-XsroDSFmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('SpaceInvaders-v0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnWCyTNWsRWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent.model.train_on_batch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfZjKGdrL6nI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Final score: \", np.mean(cumulative_avg_rewards))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHTGXAQaPKHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cumulative_avg_rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAa5WYwGPRV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "1/300000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj5f23owR1Sc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "1 -((1-0.05) * 1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mePRUsnnO7uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = np.random.randn(5,10)\n",
        "z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-e9FohjjMKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z[4,::2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqeG6r6TjYlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp9S3cHqvJar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}